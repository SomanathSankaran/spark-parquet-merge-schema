{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark parquet merge schema and partition pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/home/somanath/spark/spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_home=\"/home/somanath/spark/spark-2.2.0-bin-hadoop2.7/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "favorite_color=__HIVE_DEFAULT_PARTITION__  favorite_color=red  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls namesPartByColor.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2=spark.read.load(\"namesPartByColor.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*FileScan parquet [name#30,favorite_numbers#31,favorite_color#32] Batched: false, Format: Parquet, Location: InMemoryFileIndex[file:/home/somanath/namesPartByColor.parquet], PartitionCount: 1, PartitionFilters: [isnotnull(favorite_color#32), (favorite_color#32 = red)], PushedFilters: [], ReadSchema: struct<name:string,favorite_numbers:array<int>>\n"
     ]
    }
   ],
   "source": [
    "#only one partition is read\n",
    "df2.select(\"*\").filter(f.col(\"favorite_color\") == \"red\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*FileScan parquet [name#30,favorite_numbers#31,favorite_color#32] Batched: false, Format: Parquet, Location: InMemoryFileIndex[file:/home/somanath/namesPartByColor.parquet], PartitionCount: 2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,favorite_numbers:array<int>>\n"
     ]
    }
   ],
   "source": [
    "#all partition is read\n",
    "df2.select(\"*\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`{}examples/src/main/resources/users.parquet`\".format(spark_home))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/somanath/data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /home/somanath/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|double|single|\n",
      "+------+------+\n",
      "|     1|     1|\n",
      "|     4|     2|\n",
      "|     9|     3|\n",
      "|    16|     4|\n",
      "|    25|     5|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                      .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.show()\n",
    "squaresDF.write.parquet(\"/home/somanath/parquetmerge/test_table/key=1\")\n",
    "\n",
    "    #\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|triple|\n",
      "+------+------+\n",
      "|     6|   216|\n",
      "|     7|   343|\n",
      "|     8|   512|\n",
      "|     9|   729|\n",
      "|    10|  1000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create another DataFrame in a new partition directory,\n",
    "    # adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                    .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.show()\n",
    "cubesDF.write.parquet(\"/home/somanath/parquetmerge/test_table/key=2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|spark.sql.parquet...|false|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set  spark.sql.parquet.mergeSchema \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "|double|single|key|\n",
      "+------+------+---+\n",
      "|  null|     9|  2|\n",
      "|  null|    10|  2|\n",
      "|    16|     4|  1|\n",
      "|    25|     5|  1|\n",
      "|  null|     8|  2|\n",
      "|  null|     6|  2|\n",
      "|  null|     7|  2|\n",
      "|     4|     2|  1|\n",
      "|     1|     1|  1|\n",
      "|     9|     3|  1|\n",
      "+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/home/somanath/parquetmerge/test_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- double: long (nullable = true)\n",
      " |-- single: long (nullable = true)\n",
      " |-- triple: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n",
      "+------+------+------+---+\n",
      "|double|single|triple|key|\n",
      "+------+------+------+---+\n",
      "|  null|     9|   729|  2|\n",
      "|  null|    10|  1000|  2|\n",
      "|    16|     4|  null|  1|\n",
      "|    25|     5|  null|  1|\n",
      "|  null|     8|   512|  2|\n",
      "|  null|     6|   216|  2|\n",
      "|  null|     7|   343|  2|\n",
      "|     4|     2|  null|  1|\n",
      "|     1|     1|  null|  1|\n",
      "|     9|     3|  null|  1|\n",
      "+------+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"/home/somanath/parquetmerge/test_table\")\n",
    "mergedDF.printSchema()\n",
    "mergedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inference we can see the merge schema reading both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
